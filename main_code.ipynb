{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzP4s_6bz7Pf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYRAfBhyven6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb5srvL8z254"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdzz_uMQ1852"
      },
      "outputs": [],
      "source": [
        "# Slot dictionary for the Facebook Multilingual dataset\n",
        "slot_dict = {\n",
        " 'O': 0,\n",
        " 'B-datetime': 1,\n",
        " 'I-datetime': 2,\n",
        " 'B-alarm/alarm_modifier': 3,\n",
        " 'I-alarm/alarm_modifier': 4,\n",
        " 'B-reminder/noun': 5,\n",
        " 'I-reminder/noun': 6,\n",
        " 'B-reminder/todo': 7,\n",
        " 'I-reminder/todo': 8,\n",
        " 'B-reminder/reference': 9,\n",
        " 'I-reminder/reference': 10,\n",
        " 'B-reminder/reminder_modifier': 11,\n",
        " 'I-reminder/reminder_modifier': 12,\n",
        " 'B-reminder/recurring_period': 13,\n",
        " 'I-reminder/recurring_period': 14,\n",
        " 'B-location': 15,\n",
        " 'I-location': 16,\n",
        " 'B-weather/temperatureUnit': 17,\n",
        " 'I-weather/temperatureUnit': 18,\n",
        " 'B-weather/noun': 19,\n",
        " 'I-weather/noun': 20,\n",
        " 'B-weather/attribute': 21,\n",
        " 'I-weather/attribute': 22,\n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig3tWvht9TYY"
      },
      "outputs": [],
      "source": [
        "### Slot dictionary for the Persian-ATIS dataset\n",
        "# slot_dict = {\n",
        "#  'O': 0,\n",
        "#  'B-aircraft_code': 1,\n",
        "#  'I-aircraft_code': 2,\n",
        "#  'B-airline_code': 3,\n",
        "#  'I-airline_code': 4,\n",
        "#  'B-airline_name': 5,\n",
        "#  'I-airline_name': 6,\n",
        "#  'B-airport_code': 7,\n",
        "#  'I-airport_code': 8,\n",
        "#  'B-airport_name': 9,\n",
        "#  'I-airport_name': 10,\n",
        "#  'B-arrive_date.date_relative': 11,\n",
        "#  'I-arrive_date.date_relative': 12,\n",
        "#  'B-arrive_date.day_name': 13,\n",
        "#  'I-arrive_date.day_name': 14,\n",
        "#  'B-arrive_date.day_number': 15,\n",
        "#  'I-arrive_date.day_number': 16,\n",
        "#  'B-arrive_date.month_name': 17,\n",
        "#  'I-arrive_date.month_name': 18,\n",
        "#  'B-arrive_date.today_relative': 19,\n",
        "#  'I-arrive_date.today_relative': 20,\n",
        "#  'B-arrive_time.end_time': 21,\n",
        "#  'I-arrive_time.end_time': 22,\n",
        "#  'B-arrive_time.period_mod': 23,\n",
        "#  'I-arrive_time.period_mod': 24,\n",
        "#  'B-arrive_time.period_of_day': 25,\n",
        "#  'I-arrive_time.period_of_day': 26,\n",
        "#  'B-arrive_time.start_time': 27,\n",
        "#  'I-arrive_time.start_time': 28,\n",
        "#  'B-arrive_time.time': 29,\n",
        "#  'I-arrive_time.time': 30,\n",
        "#  'B-arrive_time.time_relative': 31,\n",
        "#  'I-arrive_time.time_relative': 32,\n",
        "#  'B-city_name': 33,\n",
        "#  'I-city_name': 34,\n",
        "#  'B-class_type': 35,\n",
        "#  'I-class_type': 36,\n",
        "#  'B-compartment': 37,\n",
        "#  'I-compartment': 38,\n",
        "#  'B-connect': 39,\n",
        "#  'I-connect': 40,\n",
        "#  'B-cost_relative': 41,\n",
        "#  'I-cost_relative': 42,\n",
        "#  'B-day_name': 43,\n",
        "#  'I-day_name': 44,\n",
        "#  'B-day_name.depart_date': 45,\n",
        "#  'I-day_name.depart_date': 46,\n",
        "#  'B-day_number': 47,\n",
        "#  'I-day_number': 48,\n",
        "#  'B-days_code': 49,\n",
        "#  'I-days_code': 50,\n",
        "#  'B-depart_date.date_relative': 51,\n",
        "#  'I-depart_date.date_relative': 52,\n",
        "#  'B-depart_date.day_name': 53,\n",
        "#  'I-depart_date.day_name': 54,\n",
        "#  'B-depart_date.day_number': 55,\n",
        "#  'I-depart_date.day_number': 56,\n",
        "#  'B-depart_date.month_name': 57,\n",
        "#  'I-depart_date.month_name': 58,\n",
        "#  'B-depart_date.time': 59,\n",
        "#  'I-depart_date.time': 60,\n",
        "#  'B-depart_date.today_relative': 61,\n",
        "#  'I-depart_date.today_relative': 62,\n",
        "#  'B-depart_date.year': 63,\n",
        "#  'I-depart_date.year': 64,\n",
        "#  'B-depart_time.end_time': 65,\n",
        "#  'I-depart_time.end_time': 66,\n",
        "#  'B-depart_time.period_mod': 67,\n",
        "#  'I-depart_time.period_mod': 68,\n",
        "#  'B-depart_time.period_of_day': 69,\n",
        "#  'I-depart_time.period_of_day': 70,\n",
        "#  'B-depart_time.start_time': 71,\n",
        "#  'I-depart_time.start_time': 72,\n",
        "#  'B-depart_time.time': 73,\n",
        "#  'I-depart_time.time': 74,\n",
        "#  'B-depart_time.time_relative': 75,\n",
        "#  'I-depart_time.time_relative': 76,\n",
        "#  'B-economy': 77,\n",
        "#  'I-economy': 78,\n",
        "#  'B-fare_amount': 79,\n",
        "#  'I-fare_amount': 80,\n",
        "#  'B-fare_basis_code': 81,\n",
        "#  'I-fare_basis_code': 82,\n",
        "#  'B-flight_days': 83,\n",
        "#  'I-flight_days': 84,\n",
        "#  'B-flight_mod': 85,\n",
        "#  'I-flight_mod': 86,\n",
        "#  'B-flight_number': 87,\n",
        "#  'I-flight_number': 88,\n",
        "#  'B-flight_stop': 89,\n",
        "#  'I-flight_stop': 90,\n",
        "#  'B-flight_time': 91,\n",
        "#  'I-flight_time': 92,\n",
        "#  'B-fromloc.airport_code': 93,\n",
        "#  'I-fromloc.airport_code': 94,\n",
        "#  'B-fromloc.airport_name': 95,\n",
        "#  'I-fromloc.airport_name': 96,\n",
        "#  'B-fromloc.city_name': 97,\n",
        "#  'I-fromloc.city_name': 98,\n",
        "#  'B-fromloc.state_code': 99,\n",
        "#  'I-fromloc.state_code': 100,\n",
        "#  'B-fromloc.state_name': 101,\n",
        "#  'I-fromloc.state_name': 102,\n",
        "#  'B-meal': 103,\n",
        "#  'I-meal': 104,\n",
        "#  'B-meal_code': 105,\n",
        "#  'I-meal_code': 106,\n",
        "#  'B-meal_description': 107,\n",
        "#  'I-meal_description': 108,\n",
        "#  'B-mod': 109,\n",
        "#  'I-mod': 110,\n",
        "#  'B-month_name': 111,\n",
        "#  'I-month_name': 112,\n",
        "#  'B-or': 113,\n",
        "#  'I-or': 114,\n",
        "#  'B-period_of_day': 115,\n",
        "#  'I-period_of_day': 116,\n",
        "#  'B-restriction_code': 117,\n",
        "#  'I-restriction_code': 118,\n",
        "#  'B-return_date.date_relative': 119,\n",
        "#  'I-return_date.date_relative': 120,\n",
        "#  'B-return_date.day_name': 121,\n",
        "#  'I-return_date.day_name': 122,\n",
        "#  'B-return_date.day_number': 123,\n",
        "#  'I-return_date.day_number': 124,\n",
        "#  'B-return_date.month_name': 125,\n",
        "#  'I-return_date.month_name': 126,\n",
        "#  'B-return_date.today_relative': 127,\n",
        "#  'I-return_date.today_relative': 128,\n",
        "#  'B-return_time.period_mod': 129,\n",
        "#  'I-return_time.period_mod': 130,\n",
        "#  'B-return_time.period_of_day': 131,\n",
        "#  'I-return_time.period_of_day': 132,\n",
        "#  'B-return_time.time': 133,\n",
        "#  'I-return_time.time': 134,\n",
        "#  'B-round_trip': 135,\n",
        "#  'I-round_trip': 136,\n",
        "#  'B-state_code': 137,\n",
        "#  'I-state_code': 138,\n",
        "#  'B-state_name': 139,\n",
        "#  'I-state_name': 140,\n",
        "#  'B-stoploc.airport_code': 141,\n",
        "#  'I-stoploc.airport_code': 142,\n",
        "#  'B-stoploc.city_name': 143,\n",
        "#  'I-stoploc.city_name': 144,\n",
        "#  'B-stoploc.state_code': 145,\n",
        "#  'I-stoploc.state_code': 146,\n",
        "#  'B-stoploc.state_name': 147,\n",
        "#  'I-stoploc.state_name': 148,\n",
        "#  'B-time': 149,\n",
        "#  'I-time': 150,\n",
        "#  'B-time_relative': 151,\n",
        "#  'I-time_relative': 152,\n",
        "#  'B-today_relative': 153,\n",
        "#  'I-today_relative': 154,\n",
        "#  'B-toloc.airport_code': 155,\n",
        "#  'I-toloc.airport_code': 156,\n",
        "#  'B-toloc.airport_name': 157,\n",
        "#  'I-toloc.airport_name': 158,\n",
        "#  'B-toloc.city_name': 159,\n",
        "#  'I-toloc.city_name': 160,\n",
        "#  'B-toloc.country_name': 161,\n",
        "#  'I-toloc.country_name': 162,\n",
        "#  'B-toloc.state_code': 163,\n",
        "#  'I-toloc.state_code': 164,\n",
        "#  'B-toloc.state_name': 165,\n",
        "#  'I-toloc.state_name': 166,\n",
        "#  'B-transport_type': 167,\n",
        "#  'I-transport_type': 168,\n",
        "#  'B-booking_class': 169,\n",
        "#  'I-booking_class': 170,\n",
        "#  'B-flight': 171,\n",
        "#  'I-flight': 172,\n",
        "#  'B-stoploc.airport_name': 173,\n",
        "#  'I-stoploc.airport_name': 174}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JALLy8Yz2eN"
      },
      "outputs": [],
      "source": [
        "def read_file(file_name):\n",
        "  path = '/content/drive/MyDrive/data/'\n",
        "  f = open(path + file_name, 'r')\n",
        "  f_list = f.readlines()\n",
        "  f.close()\n",
        "  return f_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bxVZBRw98Wp"
      },
      "source": [
        "## EN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpHwkgFK0FA7"
      },
      "outputs": [],
      "source": [
        "# EN (tokens)\n",
        "list_all_tokens_train_en = read_file('path_to_data')\n",
        "list_all_tokens_eval_en = read_file('path_to_data')\n",
        "list_all_tokens_test_en = read_file('path_to_data')\n",
        "\n",
        "# list_all_tokens_train_en = read_file('persianATIS/english-atis-tokens-train.txt')\n",
        "# list_all_tokens_eval_en = read_file('persianATIS/english-atis-tokens-eval.txt')\n",
        "# list_all_tokens_test_en = read_file('persianATIS/english-atis-tokens-test.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiM1h2Zo0b-Q"
      },
      "outputs": [],
      "source": [
        "# EN (slots)\n",
        "list_all_slots_train_en = read_file('path_to_data')\n",
        "list_all_slots_eval_en = read_file('path_to_data')\n",
        "list_all_slots_test_en = read_file('path_to_data')\n",
        "\n",
        "# list_all_slots_train_en = read_file('persianATIS/labels-english-atis-train.txt')\n",
        "# list_all_slots_eval_en = read_file('persianATIS/labels-english-atis-eval.txt')\n",
        "# list_all_slots_test_en = read_file('persianATIS/labels-english-atis-test.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2y5cEzR9-e5"
      },
      "source": [
        "## ES, FA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBF2Bk0R06nc"
      },
      "outputs": [],
      "source": [
        "# # ES (tokens)\n",
        "list_all_tokens_train_aux = read_file('es/list_token_train_es.txt')\n",
        "list_all_tokens_eval_aux = read_file('es/list_token_eval_es.txt')\n",
        "# list_all_tokens_test_aux = read_file('es/list_token_test_es.txt')\n",
        "\n",
        "# list_all_tokens_train_aux = read_file('persianATIS/persian-atis-tokens-train.txt')\n",
        "# list_all_tokens_eval_aux = read_file('persianATIS/persian-atis-tokens-eval.txt')\n",
        "# list_all_tokens_test_aux = read_file('persianATIS/persian-atis-tokens-test.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JDiqvcs1J--"
      },
      "outputs": [],
      "source": [
        "# ES (slots)\n",
        "list_all_slots_train_aux = read_file('es/label-train-es2.txt')\n",
        "list_all_slots_eval_aux = read_file('es/label-eval-es2.txt')\n",
        "# list_all_slots_test_aux = read_file('es/label-test-es2.txt')\n",
        "\n",
        "# list_all_slots_train_aux = read_file('persianATIS/labels-persian-atis-train.txt')\n",
        "# list_all_slots_eval_aux = read_file('persianATIS/labels-persian-atis-eval.txt')\n",
        "# list_all_slots_test_aux = read_file('persianATIS/labels-persian-atis-test.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWDmbv6n-Avn"
      },
      "source": [
        "## TH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph9zx4N6t8T2"
      },
      "outputs": [],
      "source": [
        "list_all_tokens_train_th = read_file('th/list_token_train_th.txt')\n",
        "list_all_slots_train_th = read_file('th/label-train-th2.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHI7lUqJ-APg"
      },
      "outputs": [],
      "source": [
        "list_all_tokens_test_th = read_file('th/list_token_test_th.txt')\n",
        "list_all_slots_test_th = read_file('th/label-test-th2.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNeEUCVl-izk"
      },
      "source": [
        "## others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeZpwEdP-klt"
      },
      "outputs": [],
      "source": [
        "list_all_tokens_train_fa = read_file('fa/list_token_train_fa.txt')\n",
        "list_all_slots_train_fa = read_file('fa/label-train-fa2.txt')\n",
        "\n",
        "list_all_tokens_test_fa = read_file('fa/list_token_test_fa.txt')\n",
        "list_all_slots_test_fa = read_file('fa/label-test-fa2.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6VWAxS0Xf2o"
      },
      "outputs": [],
      "source": [
        "list_all_tokens_train_it = read_file('en-it/tokenization-train-en-to-it.txt')\n",
        "list_all_slots_train_it = read_file('en-it/label-train-it.txt')\n",
        "\n",
        "list_all_tokens_test_it = read_file('en-it/tokenization-test-en-to-it.txt')\n",
        "list_all_slots_test_it = read_file('en-it/label-test-it.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EiFsC8_1XUG"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "def string_to_list(my_list):\n",
        "  for i in range(len(my_list)):\n",
        "    my_list[i] = ast.literal_eval(my_list[i])\n",
        "  return my_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1mGOb4N1ZeS"
      },
      "outputs": [],
      "source": [
        "# EN\n",
        "list_all_tokens_train_en = string_to_list(list_all_tokens_train_en)\n",
        "list_all_slots_train_en = string_to_list(list_all_slots_train_en)\n",
        "\n",
        "list_all_tokens_eval_en = string_to_list(list_all_tokens_eval_en)\n",
        "list_all_slots_eval_en = string_to_list(list_all_slots_eval_en)\n",
        "\n",
        "list_all_tokens_test_en = string_to_list(list_all_tokens_test_en)\n",
        "list_all_slots_test_en = string_to_list(list_all_slots_test_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O0epcae1b72"
      },
      "outputs": [],
      "source": [
        "# ES\n",
        "list_all_tokens_train_aux = string_to_list(list_all_tokens_train_aux)\n",
        "list_all_slots_train_aux = string_to_list(list_all_slots_train_aux)\n",
        "\n",
        "list_all_tokens_eval_aux = string_to_list(list_all_tokens_eval_aux)\n",
        "list_all_slots_eval_aux = string_to_list(list_all_slots_eval_aux)\n",
        "\n",
        "# list_all_tokens_test_aux = string_to_list(list_all_tokens_test_aux)\n",
        "# list_all_slots_test_aux = string_to_list(list_all_slots_test_aux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayWiz9ka-sMm"
      },
      "outputs": [],
      "source": [
        "# TH\n",
        "list_all_tokens_train_th = string_to_list(list_all_tokens_train_th)\n",
        "list_all_slots_train_th = string_to_list(list_all_slots_train_th)\n",
        "\n",
        "list_all_tokens_test_th = string_to_list(list_all_tokens_test_th)\n",
        "list_all_slots_test_th = string_to_list(list_all_slots_test_th)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81vvEZkM-xI9"
      },
      "outputs": [],
      "source": [
        "# FA\n",
        "list_all_tokens_train_fa = string_to_list(list_all_tokens_train_fa)\n",
        "list_all_slots_train_fa = string_to_list(list_all_slots_train_fa)\n",
        "\n",
        "list_all_tokens_test_fa = string_to_list(list_all_tokens_test_fa)\n",
        "list_all_slots_test_fa = string_to_list(list_all_slots_test_fa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6z7byrsX9mw"
      },
      "outputs": [],
      "source": [
        "#IT\n",
        "list_all_tokens_train_it = string_to_list(list_all_tokens_train_it)\n",
        "list_all_slots_train_it = string_to_list(list_all_slots_train_it)\n",
        "\n",
        "list_all_tokens_test_it = string_to_list(list_all_tokens_test_it)\n",
        "list_all_slots_test_it = string_to_list(list_all_slots_test_it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BijS5uvq1yA7"
      },
      "outputs": [],
      "source": [
        "def slots_to_ids(slot_dict, slot_list):\n",
        "    for i in range(len(slot_list)):\n",
        "      sl = slot_list[i]\n",
        "      for j in range(len(sl)):\n",
        "        label = sl[j]\n",
        "        # print(label)\n",
        "        if label in ['B-news/type','B-negation', 'I-negation', 'B-timer/noun', 'B-timer/attributes', 'B-demonstrative_reference', 'I-demonstrative_reference']: label = 'O'\n",
        "        if label == 'B-alarm/recurring_period': label = 'B-datetime'\n",
        "        if label == 'I-alarm/recurring_period': label = 'I-datetime'\n",
        "\n",
        "        ### persian atis ###\n",
        "        if label.startswith(' '): label = label[1:]\n",
        "        if label.startswith('\\u200c'): label = label[1:]\n",
        "        if label == 'O-depart_time.time': label = 'O'\n",
        "        if label.startswith('B.'): label = 'B-'+label[2:]\n",
        "        if label.startswith('I.'): label = 'I-'+label[2:]\n",
        "        if label == 'I-depart_date.time_relative': label = 'I-depart_time.time_relative'\n",
        "\n",
        "        label_id = slot_dict[label]\n",
        "        slot_list[i][j] = label_id\n",
        "\n",
        "    return slot_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oW-vMFpq12fU"
      },
      "outputs": [],
      "source": [
        "# EN\n",
        "list_all_slots_train_en_ids_org = slots_to_ids(slot_dict, list_all_slots_train_en)\n",
        "list_all_slots_eval_en_ids_org = slots_to_ids(slot_dict, list_all_slots_eval_en)\n",
        "list_all_slots_test_en_ids_org = slots_to_ids(slot_dict, list_all_slots_test_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgTDeu8Y14ym"
      },
      "outputs": [],
      "source": [
        "# ES\n",
        "list_all_slots_train_aux_ids_org = slots_to_ids(slot_dict, list_all_slots_train_aux)\n",
        "list_all_slots_eval_aux_ids_org = slots_to_ids(slot_dict, list_all_slots_eval_aux)\n",
        "# list_all_slots_test_aux_ids_org = slots_to_ids(slot_dict, list_all_slots_test_aux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-NDx9gd-8Yg"
      },
      "outputs": [],
      "source": [
        "# TH\n",
        "list_all_slots_train_th_ids_org = slots_to_ids(slot_dict, list_all_slots_train_th)\n",
        "list_all_slots_test_th_ids_org = slots_to_ids(slot_dict, list_all_slots_test_th)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htpzvcsk-9t9"
      },
      "outputs": [],
      "source": [
        "# FA\n",
        "list_all_slots_train_fa_ids_org = slots_to_ids(slot_dict, list_all_slots_train_fa)\n",
        "list_all_slots_test_fa_ids_org = slots_to_ids(slot_dict, list_all_slots_test_fa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fqfVrH6YB-3"
      },
      "outputs": [],
      "source": [
        "# IT\n",
        "list_all_slots_train_it_ids_org = slots_to_ids(slot_dict, list_all_slots_train_it)\n",
        "list_all_slots_test_it_ids_org = slots_to_ids(slot_dict, list_all_slots_test_it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co2sVsaaV9IJ"
      },
      "outputs": [],
      "source": [
        "intent_dict = {'alarm/set_alarm': 0,\n",
        " 'alarm/show_alarms': 1,\n",
        " 'alarm/cancel_alarm': 2,\n",
        " 'alarm/time_left_on_alarm': 3,\n",
        " 'alarm/modify_alarm': 4,\n",
        " 'alarm/snooze_alarm': 5,\n",
        " 'reminder/set_reminder': 6,\n",
        " 'reminder/show_reminders': 7,\n",
        " 'reminder/cancel_reminder': 8,\n",
        " 'weather/find': 9,\n",
        " 'weather/checkSunrise': 10,\n",
        " 'weather/checkSunset': 11}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSqYS8fu2YAX"
      },
      "outputs": [],
      "source": [
        "### Persian-ATIS ###\n",
        "# intent_dict = {\n",
        "#   'flight_no': 0,\n",
        "#   'airfare+flight': 1,\n",
        "#   'capacity': 2,\n",
        "#   'airfare+flight_time': 3,\n",
        "#   'quantity': 4,\n",
        "#   'airfare': 5,\n",
        "#   'ground_service+ground_fare': 6,\n",
        "#   'city': 7,\n",
        "#   'flight_no+airline': 8,\n",
        "#   'flight': 9,\n",
        "#   'flight+airfare': 10,\n",
        "#   'airport': 11,\n",
        "#   'abbreviation': 12,\n",
        "#   'cheapest': 13,\n",
        "#   'aircraft+flight+flight_no': 14,\n",
        "#   'distance': 15,\n",
        "#   'restriction': 16,\n",
        "#   'meal': 17,\n",
        "#   'aircraft': 18,\n",
        "#   'flight_time': 19,\n",
        "#   'flight+airline': 20,\n",
        "#   'ground_fare': 21,\n",
        "#   'airline+flight_no': 22,\n",
        "#   'airline': 23,\n",
        "#   'ground_service': 24,\n",
        "#   'day_name': 25,\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCgSE_4XV8MT"
      },
      "outputs": [],
      "source": [
        "def convert_label(file_name, intent_dict):\n",
        "  intent_list = read_file(file_name)\n",
        "  label_list = []\n",
        "  for int_ in intent_list:\n",
        "    the_intent = int_.strip()\n",
        "    label_list.append(intent_dict[the_intent])\n",
        "  return label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1GfWmg2WB5b"
      },
      "outputs": [],
      "source": [
        "# EN\n",
        "intent_label_train_en = torch.tensor(convert_label('new_en/train_en_sep_intent.txt', intent_dict))\n",
        "intent_label_eval_en = torch.tensor(convert_label('new_en/eval_en_sep_intent.txt', intent_dict))\n",
        "intent_label_test_en = torch.tensor(convert_label('new_en/test_en_sep_intent.txt', intent_dict))\n",
        "\n",
        "# intent_label_train_en = torch.tensor(convert_label('persianATIS/intent-english-atis-train.txt', intent_dict))\n",
        "# intent_label_eval_en = torch.tensor(convert_label('persianATIS/intent-english-atis-eval.txt', intent_dict))\n",
        "# intent_label_test_en = torch.tensor(convert_label('persianATIS/intent-english-atis-test.txt', intent_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgREZpFSWFGm"
      },
      "outputs": [],
      "source": [
        "# ES\n",
        "intent_label_train_aux = torch.tensor(convert_label('es/intent_train-es.txt', intent_dict))\n",
        "intent_label_eval_aux = torch.tensor(convert_label('es/intent_eval-es.txt', intent_dict))\n",
        "# intent_label_test_aux = torch.tensor(convert_label('es/intent_test-es.txt', intent_dict))\n",
        "\n",
        "# intent_label_train_aux = torch.tensor(convert_label('persianATIS/intent-persian-atis-train.txt', intent_dict))\n",
        "# intent_label_eval_aux = torch.tensor(convert_label('persianATIS/intent-persian-atis-eval.txt', intent_dict))\n",
        "# intent_label_test_aux = torch.tensor(convert_label('persianATIS/intent-persian-atis-test.txt', intent_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOzpYIFh_PE7"
      },
      "outputs": [],
      "source": [
        "# TH\n",
        "intent_label_train_th = torch.tensor(convert_label('th/intent-train-th.txt', intent_dict))\n",
        "intent_label_test_th = torch.tensor(convert_label('th/intent-test-th.txt', intent_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4GLcpuV_c9I"
      },
      "outputs": [],
      "source": [
        "# FA\n",
        "intent_label_train_fa = torch.tensor(convert_label('fa/intent_train-fa.txt', intent_dict))\n",
        "intent_label_test_fa = torch.tensor(convert_label('fa/intent_test-fa.txt', intent_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb0fz4_2YJCs"
      },
      "outputs": [],
      "source": [
        "# IT\n",
        "intent_label_train_it = torch.tensor(convert_label('new_en/train_en_sep_intent.txt', intent_dict))\n",
        "intent_label_test_it = torch.tensor(convert_label('new_en/test_en_sep_intent.txt', intent_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opcb79WuhBhW"
      },
      "source": [
        "# BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrdCmRbOUHkF"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertPreTrainedModel, BertModel, BertConfig, BertTokenizer\n",
        "\n",
        "from transformers import MBartForConditionalGeneration, MBartTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOvuh5mOmkP0"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGOSuE1LXduK"
      },
      "outputs": [],
      "source": [
        "model_name = 'facebook/mbart-large-50'  #'facebook/mbart-large-cc25'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvPDS-rwS0X7"
      },
      "outputs": [],
      "source": [
        "def bart_text_preparation(text, slot_labels, tokenizer):\n",
        "  just_check = []\n",
        "  tokenized_text = []\n",
        "  slot_labels_ids = []\n",
        "  special_token_label_id = -1\n",
        "  # O_id = 0\n",
        "  pad_token_id = tokenizer.pad_token_id # pad_token_id is 1\n",
        "  marked_text = ['<s>'] + text + ['</s>']\n",
        "  slot_labels = [special_token_label_id] + slot_labels + [special_token_label_id]\n",
        "  max_seq_len = 46  #37\n",
        "\n",
        "  for word, slot_label in zip(marked_text, slot_labels):\n",
        "    # print(word)\n",
        "    # print(slot_label)\n",
        "    word_tokens = tokenizer.tokenize(word)\n",
        "    # print(word_tokens)\n",
        "    # if not word_tokens:\n",
        "    #     word_tokens = [special_token_label_id]  # For handling the bad-encoded word\n",
        "    tokenized_text.extend(word_tokens)\n",
        "\n",
        "    # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "    first_label = int(slot_label)\n",
        "    if first_label == 0: #O\n",
        "      other_labels = 0\n",
        "    elif first_label%2 == 1: #Odd numbers: B-\n",
        "      other_labels = first_label + 1\n",
        "    else:\n",
        "      other_labels = first_label\n",
        "\n",
        "    slot_labels_ids.extend([first_label] + [other_labels] * (len(word_tokens) - 1))\n",
        "\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  attention_mask = [1]*len(indexed_tokens)\n",
        "\n",
        "  padding_length = max_seq_len - len(indexed_tokens)\n",
        "  indexed_tokens = indexed_tokens + ([pad_token_id] * padding_length)\n",
        "\n",
        "\n",
        "  attention_mask = attention_mask + [0]*padding_length\n",
        "  slot_labels_ids = slot_labels_ids + ([special_token_label_id] * padding_length)\n",
        "\n",
        "  ### truncation\n",
        "  indexed_tokens = indexed_tokens[:max_seq_len]\n",
        "  attention_mask = attention_mask[:max_seq_len]\n",
        "  slot_labels_ids = slot_labels_ids[:max_seq_len]\n",
        "\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([attention_mask])\n",
        "\n",
        "  return tokens_tensor, segments_tensors, slot_labels_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MUOAW-4YzAP"
      },
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "tokenizer = MBartTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwfXo5ru9rBp"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nInuaq549qX2"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_features): #in_features: 1024\n",
        "    super().__init__()\n",
        "    self.disc = nn.Sequential(\n",
        "      nn.Linear(in_features, 256),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(256, 128),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(128, 1), #nclass: 1\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.disc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wT5T2mAP5sx"
      },
      "outputs": [],
      "source": [
        "# class Generator(nn.Module):\n",
        "#   def __init__(self, in_features, hidden_size, num_layers):\n",
        "#     super(Generator, self).__init__()\n",
        "#     self.hidden_size = hidden_size\n",
        "#     self.num_layers = num_layers\n",
        "#     self.gen = nn.LSTM(in_features, hidden_size, num_layers, batch_first = True, bidirectional=True)\n",
        "\n",
        "#   def forward(self, x):\n",
        "\n",
        "#     h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device)\n",
        "#     c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device)\n",
        "\n",
        "#     out, _ = self.gen(x, (h0, c0))\n",
        "#     # h_out = h_out.view(-1, self.hidden_size)\n",
        "\n",
        "#     return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rKJzvbkKlZ7"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIRjRq9WY9Xl"
      },
      "outputs": [],
      "source": [
        "model = MBartForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jfb9r3tY6il"
      },
      "outputs": [],
      "source": [
        "for param in model.model.encoder.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "  modules = [model.model.encoder.embed_positions, *model.model.encoder.layers[-3:]]\n",
        "\n",
        "  for module in modules:\n",
        "    for param in module.parameters():\n",
        "      param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5R09jJoLlhQ"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od6ai-yg1vV7"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.mbart = model\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        encoder_output = self.mbart(input_ids=x, attention_mask=attention_mask)\n",
        "        token_embeddings = encoder_output['encoder_last_hidden_state']\n",
        "        return token_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIcWZ2vOXqm0"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6USgY8-LN82w"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, model):\n",
        "      super(Decoder, self).__init__()\n",
        "      self.mbart = model\n",
        "\n",
        "  def forward(self, x, encoder_hidden_states):\n",
        "      decoder_output = self.mbart.model.decoder(input_ids=x, encoder_hidden_states=encoder_hidden_states)\n",
        "      # output = decoder_output.last_hidden_state\n",
        "\n",
        "      logits = self.mbart.lm_head(decoder_output.last_hidden_state)\n",
        "      # probabs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP1OesPOrVok"
      },
      "source": [
        "# loss & opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaX2BKQlQMIp"
      },
      "outputs": [],
      "source": [
        "encoder_model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPiMU_qf27EY"
      },
      "outputs": [],
      "source": [
        "disc_ = Discriminator(1024).to(device)\n",
        "opt_disc = optim.Adam(disc_.parameters(), lr=3e-4)\n",
        "\n",
        "# gen_ = Generator(768, 256, 2).to(device)\n",
        "# opt_gen = optim.Adam(gen_.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynsiZh-w6o1m"
      },
      "outputs": [],
      "source": [
        "opt_gen = optim.Adam(list(encoder_model.parameters()) + list(slot_tagger.parameters()) + list(classifier.parameters()), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPD9QSyJnpMC"
      },
      "outputs": [],
      "source": [
        "decoder_model = Decoder(model).to(device)\n",
        "# opt_decoder = optim.Adam(decoder_model.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4m_1LD6xuS3"
      },
      "outputs": [],
      "source": [
        "reconstruction_loss = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNMX77wvyx78"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBMeqocFlFbw"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkqhek2hmTDY"
      },
      "outputs": [],
      "source": [
        "def get_tokenized_data(in_hand_data, in_hand_labels):\n",
        "  list_token = [] #list of token_ids\n",
        "  list_attention = []\n",
        "  list_slot_labels_ids = []\n",
        "  for i in range(len(in_hand_data)):\n",
        "    tokens_tensor, segments_tensors, slot_labels_ids = bart_text_preparation(in_hand_data[i], in_hand_labels[i], tokenizer)\n",
        "\n",
        "\n",
        "    list_token.append(tokens_tensor)\n",
        "    list_attention.append(segments_tensors[0])\n",
        "    list_slot_labels_ids.append(slot_labels_ids)\n",
        "\n",
        "  return list_token, list_attention, list_slot_labels_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7VcrjZeskq3"
      },
      "outputs": [],
      "source": [
        "lst_tkn_train_en, lst_att_train_en, lst_slot_train_en = get_tokenized_data(list_all_tokens_train_en, list_all_slots_train_en_ids_org)\n",
        "\n",
        "lst_tkn_eval_en, lst_att_eval_en, lst_slot_eval_en = get_tokenized_data(list_all_tokens_eval_en, list_all_slots_eval_en_ids_org)\n",
        "\n",
        "lst_tkn_test_en, lst_att_test_en, lst_slot_test_en = get_tokenized_data(list_all_tokens_test_en, list_all_slots_test_en_ids_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlXpQK5e4Cse"
      },
      "outputs": [],
      "source": [
        "lst_tkn_train_aux, lst_att_train_aux, lst_slot_train_aux = get_tokenized_data(list_all_tokens_train_aux, list_all_slots_train_aux_ids_org)\n",
        "\n",
        "lst_tkn_eval_aux, lst_att_eval_aux, lst_slot_eval_aux = get_tokenized_data(list_all_tokens_eval_aux, list_all_slots_eval_aux_ids_org)\n",
        "\n",
        "# lst_tkn_test_aux, lst_att_test_aux, lst_slot_test_aux = get_tokenized_data(list_all_tokens_test_aux, list_all_slots_test_aux_ids_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJL6MTdwCAYi"
      },
      "outputs": [],
      "source": [
        "lst_tkn_train_th, lst_att_train_th, lst_slot_train_th = get_tokenized_data(list_all_tokens_train_th, list_all_slots_train_th_ids_org)\n",
        "\n",
        "lst_tkn_test_th, lst_att_test_th, lst_slot_test_th = get_tokenized_data(list_all_tokens_test_th, list_all_slots_test_th_ids_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rsKUGfAuncp"
      },
      "outputs": [],
      "source": [
        "lst_tkn_train_fa, lst_att_train_fa, lst_slot_train_fa = get_tokenized_data(list_all_tokens_train_fa, list_all_slots_train_fa_ids_org)\n",
        "\n",
        "lst_tkn_test_fa, lst_att_test_fa, lst_slot_test_fa = get_tokenized_data(list_all_tokens_test_fa, list_all_slots_test_fa_ids_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0ILhHWkYuKY"
      },
      "outputs": [],
      "source": [
        "lst_tkn_train_it, lst_att_train_it, lst_slot_train_it = get_tokenized_data(list_all_tokens_train_it, list_all_slots_train_it_ids_org)\n",
        "\n",
        "lst_tkn_test_it, lst_att_test_it, lst_slot_test_it = get_tokenized_data(list_all_tokens_test_it, list_all_slots_test_it_ids_org)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG1sJOtuuSq9"
      },
      "outputs": [],
      "source": [
        "tensor_tkn_train_en = torch.cat([l for l in lst_tkn_train_en], dim = 0)\n",
        "tensor_tkn_eval_en = torch.cat([l for l in lst_tkn_eval_en], dim = 0)\n",
        "tensor_tkn_test_en = torch.cat([l for l in lst_tkn_test_en], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_LrO3-hwY5s"
      },
      "outputs": [],
      "source": [
        "tensor_tkn_train_aux = torch.cat([l for l in lst_tkn_train_aux], dim = 0)\n",
        "tensor_tkn_eval_aux = torch.cat([l for l in lst_tkn_eval_aux], dim = 0)\n",
        "# tensor_tkn_test_aux = torch.cat([l for l in lst_tkn_test_aux], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5gOQg4ruxDd"
      },
      "outputs": [],
      "source": [
        "tensor_tkn_train_th = torch.cat([l for l in lst_tkn_train_th], dim = 0)\n",
        "tensor_tkn_test_th = torch.cat([l for l in lst_tkn_test_th], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymZxi8GUCNDX"
      },
      "outputs": [],
      "source": [
        "tensor_tkn_train_fa = torch.cat([l for l in lst_tkn_train_fa], dim = 0)\n",
        "tensor_tkn_test_fa = torch.cat([l for l in lst_tkn_test_fa], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFwEbFWwY14_"
      },
      "outputs": [],
      "source": [
        "tensor_tkn_train_it = torch.cat([l for l in lst_tkn_train_it], dim = 0)\n",
        "tensor_tkn_test_it = torch.cat([l for l in lst_tkn_test_it], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhG7skO55Zyl"
      },
      "outputs": [],
      "source": [
        "tensor_att_train_en = torch.stack([l for l in lst_att_train_en], dim = 0)\n",
        "tensor_att_eval_en = torch.stack([l for l in lst_att_eval_en], dim = 0)\n",
        "tensor_att_test_en = torch.stack([l for l in lst_att_test_en], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpzI4yDHwb17"
      },
      "outputs": [],
      "source": [
        "tensor_att_train_aux = torch.stack([l for l in lst_att_train_aux], dim = 0)\n",
        "tensor_att_eval_aux = torch.stack([l for l in lst_att_eval_aux], dim = 0)\n",
        "# tensor_att_test_aux = torch.stack([l for l in lst_att_test_aux], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHizZBg5u6LW"
      },
      "outputs": [],
      "source": [
        "tensor_att_train_th = torch.stack([l for l in lst_att_train_th], dim = 0)\n",
        "tensor_att_test_th = torch.stack([l for l in lst_att_test_th], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYE4xGBrCVDD"
      },
      "outputs": [],
      "source": [
        "tensor_att_train_fa = torch.stack([l for l in lst_att_train_fa], dim = 0)\n",
        "tensor_att_test_fa = torch.stack([l for l in lst_att_test_fa], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqqKhWSQY5Y7"
      },
      "outputs": [],
      "source": [
        "tensor_att_train_it = torch.stack([l for l in lst_att_train_it], dim = 0)\n",
        "tensor_att_test_it = torch.stack([l for l in lst_att_test_it], dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3xM75v954cb"
      },
      "outputs": [],
      "source": [
        "train_en_slot_ids = torch.tensor(lst_slot_train_en)\n",
        "eval_en_slot_ids = torch.tensor(lst_slot_eval_en)\n",
        "test_en_slot_ids = torch.tensor(lst_slot_test_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyX0gJFMwfDi"
      },
      "outputs": [],
      "source": [
        "train_aux_slot_ids = torch.tensor(lst_slot_train_aux)\n",
        "eval_aux_slot_ids = torch.tensor(lst_slot_eval_aux)\n",
        "# test_aux_slot_ids = torch.tensor(lst_slot_test_aux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU9wJbltvAxj"
      },
      "outputs": [],
      "source": [
        "train_th_slot_ids = torch.tensor(lst_slot_train_th)\n",
        "test_th_slot_ids = torch.tensor(lst_slot_test_th)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3zyFsmcCbUS"
      },
      "outputs": [],
      "source": [
        "train_fa_slot_ids = torch.tensor(lst_slot_train_fa)\n",
        "test_fa_slot_ids = torch.tensor(lst_slot_test_fa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1RSxyXeY9HH"
      },
      "outputs": [],
      "source": [
        "train_it_slot_ids = torch.tensor(lst_slot_train_it)\n",
        "test_it_slot_ids = torch.tensor(lst_slot_test_it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-Ju66Ye6cP8"
      },
      "outputs": [],
      "source": [
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKuniFzn7ZGl"
      },
      "source": [
        "## EN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhXFgCLFwK18"
      },
      "outputs": [],
      "source": [
        "train_en_dataset = TensorDataset(tensor_tkn_train_en, tensor_att_train_en, train_en_slot_ids, intent_label_train_en)\n",
        "train_en_loader = DataLoader(train_en_dataset, batch_size=batch_size, shuffle = True)\n",
        "\n",
        "eval_en_dataset = TensorDataset(tensor_tkn_eval_en, tensor_att_eval_en, eval_en_slot_ids, intent_label_eval_en)\n",
        "eval_en_loader = DataLoader(eval_en_dataset, batch_size=batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vfaiOBL62iL"
      },
      "outputs": [],
      "source": [
        "test_en_dataset = TensorDataset(tensor_tkn_test_en, tensor_att_test_en, test_en_slot_ids, intent_label_test_en)\n",
        "bs = 1 #len(test_en_dataset)\n",
        "test_en_loader = DataLoader(test_en_dataset, batch_size=bs, shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puWbCQQX7bqE"
      },
      "source": [
        "## ES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cnrggh2xSR-"
      },
      "outputs": [],
      "source": [
        "train_aux_dataset = TensorDataset(tensor_tkn_train_aux, tensor_att_train_aux, train_aux_slot_ids, intent_label_train_aux)\n",
        "train_aux_loader = DataLoader(train_aux_dataset, batch_size=batch_size, shuffle = True)\n",
        "\n",
        "eval_aux_dataset = TensorDataset(tensor_tkn_eval_aux, tensor_att_eval_aux, eval_aux_slot_ids, intent_label_eval_aux)\n",
        "eval_aux_loader = DataLoader(eval_aux_dataset, batch_size=batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3atOu8n65Pj"
      },
      "outputs": [],
      "source": [
        "test_aux_dataset = TensorDataset(tensor_tkn_test_aux, tensor_att_test_aux, test_aux_slot_ids, intent_label_test_aux)\n",
        "bs = 1 #len(test_aux_dataset)\n",
        "test_aux_loader = DataLoader(test_aux_dataset, batch_size=bs, shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yhZ5eGcCjxs"
      },
      "source": [
        "## others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xom0IC52vJqM"
      },
      "outputs": [],
      "source": [
        "train_th_dataset = TensorDataset(tensor_tkn_train_th, tensor_att_train_th, train_th_slot_ids, intent_label_train_th)\n",
        "train_th_loader = DataLoader(train_th_dataset, batch_size=batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAzsOD4cCltg"
      },
      "outputs": [],
      "source": [
        "test_th_dataset = TensorDataset(tensor_tkn_test_th, tensor_att_test_th, test_th_slot_ids, intent_label_test_th)\n",
        "bs = 1  #len(test_th_dataset)\n",
        "test_th_loader = DataLoader(test_th_dataset, batch_size=bs, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCiTImeF3O17"
      },
      "outputs": [],
      "source": [
        "train_fa_dataset = TensorDataset(tensor_tkn_train_fa, tensor_att_train_fa, train_fa_slot_ids, intent_label_train_fa)\n",
        "train_fa_loader = DataLoader(train_fa_dataset, batch_size=batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTEiZUzeCqlu"
      },
      "outputs": [],
      "source": [
        "test_fa_dataset = TensorDataset(tensor_tkn_test_fa, tensor_att_test_fa, test_fa_slot_ids, intent_label_test_fa)\n",
        "bs = 1  #len(test_fa_dataset)\n",
        "test_fa_loader = DataLoader(test_fa_dataset, batch_size=bs, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DZm04_ZvXAb"
      },
      "outputs": [],
      "source": [
        "train_it_dataset = TensorDataset(tensor_tkn_train_it, tensor_att_train_it, train_it_slot_ids, intent_label_train_it)\n",
        "train_it_loader = DataLoader(train_it_dataset, batch_size=batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5uayNKoZBBe"
      },
      "outputs": [],
      "source": [
        "test_it_dataset = TensorDataset(tensor_tkn_test_it, tensor_att_test_it, test_it_slot_ids, intent_label_test_it)\n",
        "bs = 1  #len(test_it_dataset)\n",
        "test_it_loader = DataLoader(test_it_dataset, batch_size=bs, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPTH0XjM8aMh"
      },
      "source": [
        "# Adversarial Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNGxEVEpPCxt"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZWTJjiH9e_K"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "num_epochs = 15\n",
        "k_steps = 3\n",
        "alpha = 0.5; beta = 0.5\n",
        "d = 1\n",
        "t1 = 1; t2 = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4ILUCDH8miR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import mean\n",
        "\n",
        "L1, L2, L3, L4, L5 = [], [], [], [], []\n",
        "# L1_val, L2_val, L3_val = [], [], []\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "  encoder_model.train()\n",
        "  disc_.train()\n",
        "\n",
        "  loss_T = []; loss_D = []; loss_Dec = []\n",
        "  loss_SF, loss_ID = [], []\n",
        "  i=0\n",
        "  for batch_en, batch_aux in zip(train_en_loader, train_aux_loader):\n",
        "\n",
        "      if i<k_steps:\n",
        "\n",
        "        x_en, y_en, _, _ = batch_en\n",
        "        x_en = x_en.to(device)\n",
        "        y_en = y_en.to(device)\n",
        "\n",
        "        x_aux, y_aux, _, _ = batch_aux\n",
        "        x_aux = x_aux.to(device)\n",
        "        y_aux = y_aux.to(device)\n",
        "\n",
        "        data_gen_en = encoder_model(x_en, y_en)\n",
        "\n",
        "        data_gen_aux = encoder_model(x_aux, y_aux)\n",
        "\n",
        "        disc_gen_en = disc_(data_gen_en)\n",
        "        disc_gen_aux = disc_(data_gen_aux)\n",
        "\n",
        "        disc_.zero_grad()\n",
        "\n",
        "        loss_disc_real = criterion(disc_gen_en, torch.ones_like(disc_gen_en))\n",
        "        loss_disc_fake = criterion(disc_gen_aux, torch.zeros_like(disc_gen_en))\n",
        "        loss_disc = 0.5*(loss_disc_real + loss_disc_fake)\n",
        "        loss_D.append(loss_disc.item())\n",
        "\n",
        "        loss_disc.backward(retain_graph=True)\n",
        "\n",
        "        opt_disc.step()\n",
        "        i = i + 1\n",
        "\n",
        "      else:\n",
        "        x_en_t, y_en_t, z_en_t, w_en_t = batch_en\n",
        "        x_en_t = x_en_t.to(device)\n",
        "        y_en_t = y_en_t.to(device)\n",
        "        z_en_t = z_en_t.to(device)\n",
        "        w_en_t = w_en_t.to(device)\n",
        "\n",
        "        x_aux_t, y_aux_t, z_aux, w_aux = batch_aux\n",
        "        x_aux_t = x_aux_t.to(device)\n",
        "        y_aux_t = y_aux_t.to(device)\n",
        "        z_aux = z_aux.to(device)\n",
        "        w_aux = w_aux.to(device)\n",
        "\n",
        "        encoder_model.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          outputs = encoder_model(x_en_t, y_en_t)\n",
        "          hidden_states = outputs[2][1:]\n",
        "\n",
        "        data_gen_en_t = hidden_states[-1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "          outputs = encoder_model(x_aux_t, y_aux_t)\n",
        "          hidden_states = outputs[2][1:]\n",
        "\n",
        "        data_gen_aux_t = hidden_states[-1]\n",
        "\n",
        "        disc_gen_en_t = disc_(data_gen_en_t)\n",
        "        disc_gen_aux_t = disc_(data_gen_aux_t)\n",
        "        ### Disc. loss:\n",
        "        loss_disc_real_t= criterion(disc_gen_en_t, torch.ones_like(disc_gen_en_t))\n",
        "        loss_disc_fake_t = criterion(disc_gen_aux_t, torch.zeros_like(disc_gen_aux_t))\n",
        "        loss_disc_t = 0.5*(loss_disc_real_t + loss_disc_fake_t)\n",
        "        loss_D.append(loss_disc_t.item())\n",
        "        # ## Decoder loss:\n",
        "        decoder_output_en = decoder_model(x_en_t, data_gen_en_t)\n",
        "        decoder_output_aux = decoder_model(x_aux_t, data_gen_aux_t)\n",
        "\n",
        "        r_loss_en = reconstruction_loss(torch.transpose(decoder_output_en, 1, 2), x_en_t)\n",
        "        r_loss_aux = reconstruction_loss(torch.transpose(decoder_output_aux, 1, 2), x_aux_t)\n",
        "        r_loss = alpha*r_loss_en + beta*r_loss_aux\n",
        "        loss_Dec.append(r_loss.item())\n",
        "        ###\n",
        "        d_loss = criterion(disc_gen_aux_t, torch.ones_like(disc_gen_aux_t))\n",
        "        # loss_total = (1/4)*(d*d_loss + t1*sf_loss + t2*id_loss + r*r_loss)\n",
        "        # loss_total = (1/3)*(d*d_loss + t1*sf_loss + t2*id_loss)\n",
        "        # loss_total = 0.5*sf_loss + 0.5*id_loss - loss_disc_t\n",
        "        loss_total = 0.5*(d_loss + r_loss)\n",
        "\n",
        "        loss_T.append(loss_total.item())\n",
        "\n",
        "        loss_total.backward()\n",
        "\n",
        "        opt_gen.step()\n",
        "        i=0\n",
        "\n",
        "  loss_T_per_epoch = mean(loss_T); L1.append(loss_T_per_epoch)\n",
        "  loss_D_per_epoch = mean(loss_D); L2.append(loss_D_per_epoch)\n",
        "  loss_Dec_per_epoch = mean(loss_Dec); L5.append(loss_Dec_per_epoch)\n",
        "\n",
        "  print(f'Epoch {epoch}, \\\n",
        "  train_loss_total:{loss_T_per_epoch: .5f},\\\n",
        "  train_loss_disc:{loss_D_per_epoch: .5f},\\\n",
        "  train_loss_Dec: {loss_Dec_per_epoch}')\n",
        "\n",
        "\n",
        "#################################################################\n",
        "#################################################################\n",
        "  # if (epoch + 1) % 12 == 0: #save on 11, 23, 35, etc.\n",
        "  #   save_gan_model(encoder_model, disc_, decoder_model, opt_gen, opt_disc, opt_decoder, epoch, 'checkpoint'+str(epoch)+'.pth')\n",
        "#################################################################\n",
        "#################################################################\n",
        "  ##### Validation ##### not completed for this scenario\n",
        "  # encoder_model.eval()\n",
        "  # decoder_model.eval()\n",
        "  # disc_.eval()\n",
        "\n",
        "  # val_loss_T = []; val_loss_D = []; val_loss_Dec = []\n",
        "\n",
        "  # with torch.no_grad():\n",
        "\n",
        "  #   for batch_en_val, batch_aux_val in zip(eval_en_loader, eval_aux_loader):\n",
        "\n",
        "  #       x_en_val, y_en_val, _, _ = batch_en_val\n",
        "  #       x_en_val = x_en_val.to(device)\n",
        "  #       y_en_val = y_en_val.to(device)\n",
        "\n",
        "  #       x_aux_val, y_aux_val, _, _ = batch_aux_val\n",
        "  #       x_aux_val = x_aux_val.to(device)\n",
        "  #       y_aux_val = y_aux_val.to(device)\n",
        "\n",
        "  #       data_gen_en_val = encoder_model(x_en_val, y_en_val)\n",
        "  #       data_gen_aux_val = encoder_model(x_aux_val, y_aux_val)\n",
        "\n",
        "  #       disc_gen_en_val = disc_(data_gen_en_val)\n",
        "  #       disc_gen_aux_val = disc_(data_gen_aux_val)\n",
        "\n",
        "  #       loss_disc_real_val = criterion(disc_gen_en_val, torch.ones_like(disc_gen_en_val))\n",
        "  #       loss_disc_fake_val = criterion(disc_gen_aux_val, torch.zeros_like(disc_gen_aux_val))\n",
        "  #       loss_disc_val = 0.5 * (loss_disc_real_val + loss_disc_fake_val)\n",
        "  #       val_loss_D.append(loss_disc_val.item())\n",
        "\n",
        "  #       decoder_output_en_val = decoder_model(x_en_val, data_gen_en_val)\n",
        "  #       decoder_output_aux_val = decoder_model(x_aux_val, data_gen_aux_val)\n",
        "\n",
        "  #       r_loss_en_val = reconstruction_loss(torch.transpose(decoder_output_en_val, 1, 2), x_en_val)\n",
        "  #       r_loss_aux_val = reconstruction_loss(torch.transpose(decoder_output_aux_val, 1, 2), x_aux_val)\n",
        "  #       r_loss_val = alpha * r_loss_en_val + beta * r_loss_aux_val\n",
        "  #       val_loss_Dec.append(r_loss_val.item())\n",
        "\n",
        "  #       val_loss_total = eta * r_loss_val - lambda_coef * loss_disc_val\n",
        "  #       val_loss_T.append(val_loss_total.item())\n",
        "\n",
        "  # val_loss_T_per_epoch = mean(val_loss_T); L1_val.append(val_loss_T_per_epoch)\n",
        "  # val_loss_D_per_epoch = mean(val_loss_D); L2_val.append(val_loss_D_per_epoch)\n",
        "  # val_loss_Dec_per_epoch = mean(val_loss_Dec); L3_val.append(val_loss_Dec_per_epoch)\n",
        "\n",
        "  # print(f'Epoch {epoch}, \\\n",
        "  # eval_loss_total:{val_loss_T_per_epoch: .5f},\\\n",
        "  # eval_loss_disc:{val_loss_D_per_epoch: .5f},\\\n",
        "  # eval_loss_dec.:{val_loss_Dec_per_epoch: .5f}')\n",
        "\n",
        "  print('*****')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIg8QWTo7qqS"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHTGm9Dg7pQk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(L1, \".-b\", label=\"loss_total\")\n",
        "plt.plot(L2, \"-<r\", label=\"loss_disc\")\n",
        "\n",
        "ax = plt.gca()\n",
        "plt.xlabel(\"epoch\")  # Add x-axis label\n",
        "plt.legend(loc=\"center right\")\n",
        "# plt.subplots_adjust(right=0.8, top=0.9)\n",
        "# plt.legend(loc=\"upper right\")\n",
        "# plt.savefig('loss.jpeg', dpi=1000, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hrHmaZQ9AfG"
      },
      "source": [
        "# Slot Filling and Intent Detection classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH413GgFF9Ho"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self, in_features, hidden_size, num_layers, tagset_size): #in_feautures: img_dim #, tagset_size\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.tagset_size = tagset_size\n",
        "\n",
        "    self.lstm = nn.LSTM(in_features, hidden_size, num_layers, batch_first = True, bidirectional=True)\n",
        "    self.hidden2tag = nn.Linear(hidden_size*2, tagset_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device)\n",
        "    c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device)\n",
        "\n",
        "    out, _ = self.lstm(x, (h0, c0))\n",
        "    tag_space = self.hidden2tag(out) #out.view(x.size(1), -1)\n",
        "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "\n",
        "    return tag_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W_sB_1vF_Ob"
      },
      "outputs": [],
      "source": [
        "class IntentClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(1024, 64) #512\n",
        "    self.l2 = nn.Linear(64, 64)\n",
        "    self.l3 = nn.Linear(64, 12) #number of intents: 12, 26\n",
        "    self.do = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h1 = nn.functional.relu(self.l1(x))\n",
        "    h2 = nn.functional.relu(self.l2(h1))\n",
        "    do = self.do(h2 + h1)\n",
        "    logits = self.l3(do)\n",
        "    # probabs = F.softmax(logits, dim=-1)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzrJz8Y0Glb_"
      },
      "outputs": [],
      "source": [
        "# Slot Filling\n",
        "in_features = 1024 #512\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "tagset_size = len(slot_dict)\n",
        "slot_tagger = LSTMTagger(in_features, hidden_size, num_layers, tagset_size).to(device)\n",
        "loss_sf = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "# opt_sf = optim.SGD(slot_tagger.parameters(), lr=2e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OGaE69oHA_Q"
      },
      "outputs": [],
      "source": [
        "# Intent Detection (ID)\n",
        "classifier = IntentClassifier().to(device)\n",
        "params_id = classifier.parameters()\n",
        "loss_id = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "# opt_id = optim.SGD(params_id, lr=2e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DOm43YwHi4k"
      },
      "outputs": [],
      "source": [
        "def extract_cls(L): #L: the output of Generator\n",
        "  x_cls = [L[i][0] for i in range(len(L))]\n",
        "  final_x = torch.stack(x_cls, dim=0)\n",
        "  return final_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPgQJLqxxLWY"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn2NnXFsyUzw"
      },
      "outputs": [],
      "source": [
        "def convert_label_id_to_name_true_(list_of_labels, slot_dict, ignore_index):\n",
        "  name_list = []\n",
        "  for L in list_of_labels:\n",
        "    name_list.append([list(slot_dict.keys())[list(slot_dict.values()).index(i)] for i in L if i != ignore_index])\n",
        "  return name_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH1Yww5R0Y7X"
      },
      "outputs": [],
      "source": [
        "def convert_label_id_to_name_predict_(predict_labels, true_labels):\n",
        "  predictions = []\n",
        "  for prediction, label in zip(predict_labels, true_labels):\n",
        "    ll = []\n",
        "    for (p, l) in zip(prediction, label):\n",
        "      # print(p)\n",
        "      # print(l)\n",
        "      if p == 23: p = 0\n",
        "      if l != -1:\n",
        "        ll.append(list(slot_dict.keys())[list(slot_dict.values()).index(p)])\n",
        "        # print(list(slot_dict.keys())[list(slot_dict.values()).index(p)])\n",
        "    predictions.append(ll)\n",
        "    # print(ll, len(ll))\n",
        "  return(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlsGZFRPxOr8"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpjBU2ezxQ00"
      },
      "outputs": [],
      "source": [
        "from seqeval.metrics import classification_report\n",
        "# from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l3bhw-FIrQh"
      },
      "source": [
        "# Fine-tune SF and ID only by English data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqoyfQkMvjhv"
      },
      "outputs": [],
      "source": [
        "epoch_num = 10\n",
        "sf_coef = 0.5\n",
        "id_coef = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmCU9qRNE6St"
      },
      "outputs": [],
      "source": [
        "from numpy import mean\n",
        "loss_epoch_encoder_lst, loss_epoch_sf_lst, loss_epoch_id_lst = [], [], []\n",
        "\n",
        "for j in range(epoch_num):\n",
        "\n",
        "  loss_encoder_lst, loss_sf_lst, loss_id_lst = [], [], []\n",
        "  slot_tagger.train()\n",
        "  classifier.train()\n",
        "  encoder_model.train()\n",
        "\n",
        "  for batch in train_en_loader:\n",
        "    x, y, z, w = batch\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    z = z.to(device)\n",
        "    w = w.to(device)\n",
        "\n",
        "    g_x = encoder_model(x, y)\n",
        "    sf_x = slot_tagger(g_x)\n",
        "\n",
        "    id_x = classifier(extract_cls(g_x))\n",
        "\n",
        "    l_sf = torch.transpose(sf_x, 1, 2)\n",
        "    # l_id = torch.transpose(id_x, 1, 2)\n",
        "\n",
        "    encoder_model.zero_grad()\n",
        "    slot_tagger.zero_grad()\n",
        "    classifier.zero_grad()\n",
        "\n",
        "    J_sf = loss_sf(l_sf, z)\n",
        "    J_id = loss_id(id_x, w)\n",
        "    J = sf_coef*J_sf + id_coef*J_id\n",
        "\n",
        "    J.backward() #retain_graph = True\n",
        "    opt_gen.step()\n",
        "    #\n",
        "    # slot_tagger.zero_grad()\n",
        "    # g_x = encoder_model(x, y)\n",
        "    # sf_x = slot_tagger(g_x)\n",
        "    # l_sf = torch.transpose(sf_x, 1, 2)\n",
        "    # J_sf = loss_sf(l_sf, z)\n",
        "    # J_sf.backward()\n",
        "    # opt_sf.step()\n",
        "    # #\n",
        "    # classifier.zero_grad()\n",
        "    # g_x = encoder_model(x, y)\n",
        "    # id_x = classifier(extract_cls(g_x))\n",
        "    # J_id = loss_id(id_x, w)\n",
        "    # J_id.backward()\n",
        "    # opt_id.step()\n",
        "\n",
        "    loss_encoder_lst.append(J.item())\n",
        "    loss_sf_lst.append(J_sf.item())\n",
        "    loss_id_lst.append(J_id.item())\n",
        "\n",
        "  loss_epoch_encoder = mean(loss_encoder_lst)\n",
        "  loss_epoch_encoder_lst.append(loss_epoch_encoder)\n",
        "  #\n",
        "  loss_epoch_sf = mean(loss_sf_lst)\n",
        "  loss_epoch_sf_lst.append(loss_epoch_sf)\n",
        "  #\n",
        "  loss_epoch_id = mean(loss_id_lst)\n",
        "  loss_epoch_id_lst.append(loss_epoch_id)\n",
        "\n",
        "  print(loss_epoch_encoder, loss_epoch_sf, loss_epoch_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDUcL8NMIgcT"
      },
      "source": [
        "# Test (finally without training SF and ID) - EN, ES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPV2OO1tetxF"
      },
      "outputs": [],
      "source": [
        "all_sl_true_en = torch.tensor([], dtype=torch.long)\n",
        "all_sl_pred_en = torch.tensor([], dtype=torch.long)\n",
        "all_int_true_en = torch.tensor([], dtype=torch.long)\n",
        "all_int_pred_en = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "all_sl_true_aux = torch.tensor([], dtype=torch.long)\n",
        "all_sl_pred_aux = torch.tensor([], dtype=torch.long)\n",
        "all_int_true_aux = torch.tensor([], dtype=torch.long)\n",
        "all_int_pred_aux = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "ii = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    slot_tagger.eval()\n",
        "    classifier.eval()\n",
        "    gen_.eval()\n",
        "\n",
        "    for batch_en, batch_aux in zip(test_en_loader, test_aux_loader):\n",
        "        ii = ii + 1\n",
        "        x_en, y_en, sl_en, int_en = batch_en\n",
        "        x_en = x_en.to(device)\n",
        "        y_en = y_en.to(device)\n",
        "        sl_en = sl_en.to(device)\n",
        "        int_en = int_en.to(device)\n",
        "\n",
        "        x_aux, y_aux, sl_aux, int_aux = batch_aux\n",
        "        x_aux = x_aux.to(device)\n",
        "        y_aux = y_aux.to(device)\n",
        "        sl_aux = sl_aux.to(device)\n",
        "        int_aux = int_aux.to(device)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          outputs = encoder_model(x_en, y_en)\n",
        "          hidden_states = outputs[2][1:]\n",
        "\n",
        "        data_enc_en = hidden_states[-1]\n",
        "\n",
        "        g_test_en = gen_(data_enc_en)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          outputs = encoder_model(x_aux, y_aux)\n",
        "          hidden_states = outputs[2][1:]\n",
        "\n",
        "        data_enc_aux = hidden_states[-1]\n",
        "\n",
        "        g_test_aux = gen_(data_enc_aux)\n",
        "\n",
        "\n",
        "        out_test_en = slot_tagger(g_test_en)\n",
        "        out_test_aux = slot_tagger(g_test_aux)\n",
        "        ####\n",
        "        out_test_en_ID = classifier(extract_cls(g_test_en))\n",
        "        out_test_aux_ID = classifier(extract_cls(g_test_aux))\n",
        "        ####\n",
        "\n",
        "        # # Append predictions and true labels to accumulated tensors\n",
        "        all_sl_true_en = torch.cat((all_sl_true_en.to('cpu'), sl_en.to('cpu')), dim=0)\n",
        "        all_sl_pred_en = torch.cat((all_sl_pred_en.to('cpu'), out_test_en.to('cpu')), dim=0)\n",
        "        all_int_true_en = torch.cat((all_int_true_en.to('cpu'), int_en.to('cpu')), dim=0)\n",
        "        all_int_pred_en = torch.cat((all_int_pred_en.to('cpu'), out_test_en_ID.to('cpu')), dim=0)\n",
        "\n",
        "        all_sl_true_aux = torch.cat((all_sl_true_aux.to('cpu'), sl_aux.to('cpu')), dim=0)\n",
        "        all_sl_pred_aux = torch.cat((all_sl_pred_aux.to('cpu'), out_test_aux.to('cpu')), dim=0)\n",
        "        all_int_true_aux = torch.cat((all_int_true_aux.to('cpu'), int_aux.to('cpu')), dim=0)\n",
        "        all_int_pred_aux = torch.cat((all_int_pred_aux.to('cpu'), out_test_aux_ID.to('cpu')), dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58RnLMYPwzb1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Convert tensors to numpy arrays\n",
        "true_labels_en = all_int_true_en.numpy()\n",
        "predicted_labels_en = all_int_pred_en.numpy().argmax(axis=1)\n",
        "\n",
        "# Calculate precision, recall, and accuracy\n",
        "precision_en = precision_score(true_labels_en, predicted_labels_en, average='micro')\n",
        "precision_en_m = precision_score(true_labels_en, predicted_labels_en, average='macro')\n",
        "recall_en = recall_score(true_labels_en, predicted_labels_en, average='micro')\n",
        "recall_en_m = recall_score(true_labels_en, predicted_labels_en, average='macro')\n",
        "accuracy_en = accuracy_score(true_labels_en, predicted_labels_en)\n",
        "\n",
        "# Print the results\n",
        "print(f'Precision: {precision_en, precision_en_m}')\n",
        "print(f'Recall: {recall_en, recall_en_m}')\n",
        "print(f'Accuracy: {accuracy_en}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lRSLio5KNT2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "true_labels_aux = all_int_true_aux.numpy()\n",
        "predicted_labels_aux = all_int_pred_aux.numpy().argmax(axis=1)\n",
        "\n",
        "precision_aux = precision_score(true_labels_aux, predicted_labels_aux, average='micro')\n",
        "precision_aux_m = precision_score(true_labels_aux, predicted_labels_aux, average='macro')\n",
        "\n",
        "recall_aux = recall_score(true_labels_aux, predicted_labels_aux, average='micro')\n",
        "recall_aux_m = recall_score(true_labels_aux, predicted_labels_aux, average='macro')\n",
        "\n",
        "accuracy_aux = accuracy_score(true_labels_aux, predicted_labels_aux)\n",
        "accuracy_aux = accuracy_score(true_labels_aux, predicted_labels_aux)\n",
        "\n",
        "# Print the results\n",
        "print(f'Precision: {precision_aux, precision_aux_m}')\n",
        "print(f'Recall: {recall_aux, recall_aux_m}')\n",
        "print(f'Accuracy: {accuracy_aux}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viSvcfF6ctPR"
      },
      "outputs": [],
      "source": [
        "true_labels = all_sl_true_en.tolist()\n",
        "predict_labels = all_sl_pred_en.detach().argmax(dim=2).tolist()\n",
        "true_labels_names = convert_label_id_to_name_true_(true_labels , slot_dict, -1)\n",
        "predict_labels_names = convert_label_id_to_name_predict_(predict_labels, true_labels)\n",
        "print(classification_report(true_labels_names, predict_labels_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqD797NCct34"
      },
      "outputs": [],
      "source": [
        "true_labels_aux = all_sl_true_aux.tolist()\n",
        "predict_labels_aux = all_sl_pred_aux.detach().argmax(dim=2).tolist()\n",
        "true_labels_names_aux = convert_label_id_to_name_true_(true_labels_aux , slot_dict, -1)\n",
        "predict_labels_names_aux = convert_label_id_to_name_predict_(predict_labels_aux, true_labels_aux)\n",
        "print(classification_report(true_labels_names_aux, predict_labels_names_aux))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxjqiToVAcx_"
      },
      "source": [
        "# Test (finally without training SF and ID) - TH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygucYxWUAcyG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "all_sl_true_th = torch.tensor([], dtype=torch.long)\n",
        "all_sl_pred_th = torch.tensor([], dtype=torch.long)\n",
        "all_int_true_th = torch.tensor([], dtype=torch.long)\n",
        "all_int_pred_th = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "ii = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    slot_tagger.eval()\n",
        "    classifier.eval()\n",
        "    encoder_model.eval()\n",
        "    for batch_th in test_th_loader:\n",
        "        ii = ii + 1\n",
        "\n",
        "        x_th, y_th, sl_th, int_th = batch_th\n",
        "        x_th = x_th.to(device)\n",
        "        y_th = y_th.to(device)\n",
        "        sl_th = sl_th.to(device)\n",
        "        int_th = int_th.to(device)\n",
        "\n",
        "        g_test_th = encoder_model(x_th, y_th)\n",
        "        # g_test_th = gen_(g_test_th)\n",
        "\n",
        "        out_test_th = slot_tagger(g_test_th)\n",
        "        ####\n",
        "        out_test_th_ID = classifier(extract_cls(g_test_th))\n",
        "        ####\n",
        "\n",
        "        all_sl_true_th = torch.cat((all_sl_true_th.to('cpu'), sl_th.to('cpu')), dim=0)\n",
        "        all_sl_pred_th = torch.cat((all_sl_pred_th.to('cpu'), out_test_th.to('cpu')), dim=0)\n",
        "        all_int_true_th = torch.cat((all_int_true_th.to('cpu'), int_th.to('cpu')), dim=0)\n",
        "        all_int_pred_th = torch.cat((all_int_pred_th.to('cpu'), out_test_th_ID.to('cpu')), dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmnU2AGRAcyG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "true_labels_th = all_int_true_th.numpy()\n",
        "predicted_labels_th = all_int_pred_th.numpy().argmax(axis=1)\n",
        "\n",
        "precision_th = precision_score(true_labels_th, predicted_labels_th, average='micro')\n",
        "precision_th_m = precision_score(true_labels_th, predicted_labels_th, average='macro')\n",
        "\n",
        "recall_th = recall_score(true_labels_th, predicted_labels_th, average='micro')\n",
        "recall_th_m = recall_score(true_labels_th, predicted_labels_th, average='macro')\n",
        "\n",
        "accuracy_th = accuracy_score(true_labels_th, predicted_labels_th)\n",
        "accuracy_th = accuracy_score(true_labels_th, predicted_labels_th)\n",
        "\n",
        "# Print the results\n",
        "print(f'Precision: {precision_th, precision_th_m}')\n",
        "print(f'Recall: {recall_th, recall_th_m}')\n",
        "print(f'Accuracy: {accuracy_th}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMkQk34mAcyH"
      },
      "outputs": [],
      "source": [
        "true_labels_th = all_sl_true_th.tolist()\n",
        "predict_labels_th = all_sl_pred_th.detach().argmax(dim=2).tolist()\n",
        "true_labels_names_th = convert_label_id_to_name_true_(true_labels_th , slot_dict, -1)\n",
        "predict_labels_names_th = convert_label_id_to_name_predict_(predict_labels_th, true_labels_th)\n",
        "print(classification_report(true_labels_names_th, predict_labels_names_th))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpZOcuzYBh1t"
      },
      "source": [
        "# Test (finally without training SF and ID) - FA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH_8B_4sBh1t"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "all_sl_true_fa = torch.tensor([], dtype=torch.long)\n",
        "all_sl_pred_fa = torch.tensor([], dtype=torch.long)\n",
        "all_int_true_fa = torch.tensor([], dtype=torch.long)\n",
        "all_int_pred_fa = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "ii = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    slot_tagger.eval()\n",
        "    classifier.eval()\n",
        "    encoder_model.eval()\n",
        "    for batch_fa in test_fa_loader:\n",
        "        ii = ii + 1\n",
        "\n",
        "        x_fa, y_fa, sl_fa, int_fa = batch_fa\n",
        "        x_fa = x_fa.to(device)\n",
        "        y_fa = y_fa.to(device)\n",
        "        sl_fa = sl_fa.to(device)\n",
        "        int_fa = int_fa.to(device)\n",
        "\n",
        "        g_test_fa = encoder_model(x_fa, y_fa)\n",
        "        # g_test_fa = gen_(g_test_fa)\n",
        "\n",
        "        out_test_fa = slot_tagger(g_test_fa)\n",
        "        ####\n",
        "        out_test_fa_ID = classifier(extract_cls(g_test_fa))\n",
        "        ####\n",
        "\n",
        "        all_sl_true_fa = torch.cat((all_sl_true_fa.to('cpu'), sl_fa.to('cpu')), dim=0)\n",
        "        all_sl_pred_fa = torch.cat((all_sl_pred_fa.to('cpu'), out_test_fa.to('cpu')), dim=0)\n",
        "        all_int_true_fa = torch.cat((all_int_true_fa.to('cpu'), int_fa.to('cpu')), dim=0)\n",
        "        all_int_pred_fa = torch.cat((all_int_pred_fa.to('cpu'), out_test_fa_ID.to('cpu')), dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPe83ES-Bh1t"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "true_labels_fa = all_int_true_fa.numpy()\n",
        "predicted_labels_fa = all_int_pred_fa.numpy().argmax(axis=1)\n",
        "\n",
        "precision_fa = precision_score(true_labels_fa, predicted_labels_fa, average='micro')\n",
        "precision_fa_m = precision_score(true_labels_fa, predicted_labels_fa, average='macro')\n",
        "\n",
        "recall_fa = recall_score(true_labels_fa, predicted_labels_fa, average='micro')\n",
        "recall_fa_m = recall_score(true_labels_fa, predicted_labels_fa, average='macro')\n",
        "\n",
        "accuracy_fa = accuracy_score(true_labels_fa, predicted_labels_fa)\n",
        "accuracy_fa = accuracy_score(true_labels_fa, predicted_labels_fa)\n",
        "\n",
        "print(f'Precision: {precision_fa, precision_fa_m}')\n",
        "print(f'Recall: {recall_fa, recall_fa_m}')\n",
        "print(f'Accuracy: {accuracy_fa}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Krp6ZnaBh1u"
      },
      "outputs": [],
      "source": [
        "true_labels_fa = all_sl_true_fa.tolist()\n",
        "predict_labels_fa = all_sl_pred_fa.detach().argmax(dim=2).tolist()\n",
        "true_labels_names_fa = convert_label_id_to_name_true_(true_labels_fa , slot_dict, -1)\n",
        "predict_labels_names_fa = convert_label_id_to_name_predict_(predict_labels_fa, true_labels_fa)\n",
        "print(classification_report(true_labels_names_fa, predict_labels_names_fa))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uo7M2qwZJaJ"
      },
      "source": [
        "# Test (finally without training SF and ID) - IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT3_XfVbZJaK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "all_sl_true_it = torch.tensor([], dtype=torch.long)\n",
        "all_sl_pred_it = torch.tensor([], dtype=torch.long)\n",
        "all_int_true_it = torch.tensor([], dtype=torch.long)\n",
        "all_int_pred_it = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "ii = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    slot_tagger.eval()\n",
        "    classifier.eval()\n",
        "    encoder_model.eval()\n",
        "    for batch_it in test_it_loader:\n",
        "        ii = ii + 1\n",
        "\n",
        "        x_it, y_it, sl_it, int_it = batch_it\n",
        "        x_it = x_it.to(device)\n",
        "        y_it = y_it.to(device)\n",
        "        sl_it = sl_it.to(device)\n",
        "        int_it = int_it.to(device)\n",
        "\n",
        "        g_test_it = encoder_model(x_it, y_it)\n",
        "        # g_test_it = gen_(g_test_it)\n",
        "\n",
        "        out_test_it = slot_tagger(g_test_it)\n",
        "        ####\n",
        "        out_test_it_ID = classifier(extract_cls(g_test_it))\n",
        "        ####\n",
        "\n",
        "        all_sl_true_it = torch.cat((all_sl_true_it.to('cpu'), sl_it.to('cpu')), dim=0)\n",
        "        all_sl_pred_it = torch.cat((all_sl_pred_it.to('cpu'), out_test_it.to('cpu')), dim=0)\n",
        "        all_int_true_it = torch.cat((all_int_true_it.to('cpu'), int_it.to('cpu')), dim=0)\n",
        "        all_int_pred_it = torch.cat((all_int_pred_it.to('cpu'), out_test_it_ID.to('cpu')), dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1jYF_1-ZJaK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "true_labels_it = all_int_true_it.numpy()\n",
        "predicted_labels_it = all_int_pred_it.numpy().argmax(axis=1)\n",
        "\n",
        "precision_it = precision_score(true_labels_it, predicted_labels_it, average='micro')\n",
        "precision_it_m = precision_score(true_labels_it, predicted_labels_it, average='macro')\n",
        "\n",
        "recall_it = recall_score(true_labels_it, predicted_labels_it, average='micro')\n",
        "recall_it_m = recall_score(true_labels_it, predicted_labels_it, average='macro')\n",
        "\n",
        "accuracy_it = accuracy_score(true_labels_it, predicted_labels_it)\n",
        "accuracy_it = accuracy_score(true_labels_it, predicted_labels_it)\n",
        "\n",
        "print(f'Precision: {precision_it, precision_it_m}')\n",
        "print(f'Recall: {recall_it, recall_it_m}')\n",
        "print(f'Accuracy: {accuracy_it}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnoiDnXPZJaK"
      },
      "outputs": [],
      "source": [
        "true_labels_it = all_sl_true_it.tolist()\n",
        "predict_labels_it = all_sl_pred_it.detach().argmax(dim=2).tolist()\n",
        "true_labels_names_it = convert_label_id_to_name_true_(true_labels_it , slot_dict, -1)\n",
        "predict_labels_names_it = convert_label_id_to_name_predict_(predict_labels_it, true_labels_it)\n",
        "print(classification_report(true_labels_names_it, predict_labels_names_it))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kDUcL8NMIgcT",
        "vxjqiToVAcx_",
        "BpZOcuzYBh1t",
        "-Uo7M2qwZJaJ"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}